% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/icnet_cat.R
\name{fsnet_cat}
\alias{fsnet_cat}
\title{lastic-net penalized regression for categorical responses}
\usage{
fsnet_cat(
  Y,
  X = NULL,
  lam = 1e-05,
  alpha = 0,
  pen_factor = NULL,
  b = NULL,
  gam = NULL,
  box_constr = NULL,
  L = 10,
  maxit = rep(100, 3),
  tol = rep(1e-08, 2),
  method = "prox_newt",
  distr = "norm",
  verbose = FALSE,
  acc = TRUE,
  nfold = 1
)
}
\arguments{
\item{Y}{Vector with a categorical (factor) response with \eqn{m \geq 2}
levels.}

\item{X}{Model matrix (\eqn{n\times p}, do not include intercept!)}

\item{lam}{Vector of penalty parameters \eqn{\lambda}.}

\item{alpha}{Scalar weight \eqn{\alpha} for elastic net (1 = lasso, 0 =
ridge).}

\item{pen_factor}{Vector (\eqn{d \times 1}) of coefficient-specific penalty
weights. The first \eqn{m - 1} elements are for \eqn{\gamma} and default to
zero. The remaining \eqn{p} elements are for \eqn{\beta} and default to
one.}

\item{b}{Vector of initial values for regression coefficients \eqn{\beta}.}

\item{gam}{Vector of initial iterates for cut-off points \eqn{\gamma}.}

\item{box_constr}{Matrix (\eqn{d \times 2}) of box constraints. Can be
\code{-Inf} (first col.) or \code{Inf} (second col.).}

\item{L}{Scalar setting step-size 1 / L if \code{method = "fista"}.}

\item{maxit}{Vector of maximum number of iterations. If \code{method =
"prox_newt"}, \code{maxit[1]} for Newton, \code{maxit[2]} for linesearch,
and \code{maxit[3]} for coordinate descent within each Newton step.}

\item{tol}{Vector of tolerances for terminating algorithm. If \code{method =
"prox_newt"}, \code{tol[1]} is for Newton and \code{tol[2]} for coordinate
descent within Newton.}

\item{method}{Method to use; \code{"fista"} or \code{"prox_newt"} (proximal
Newton).}

\item{distr}{Distribution function \eqn{R} (see details); \code{"ee"} for
extreme-value or \code{"norm"} for normal.}

\item{verbose}{Logical indicating whether additional information should be
printed during fitting.}

\item{acc}{Logical indicating whether to use acceleration if \code{method =
"fista"}.}

\item{nfold}{Number of folds in \eqn{k}-fold cross-validation; 1 corresponds
to no cross-validation.}
}
\value{
If \code{nfold = 1}, a list with components
 
 \item{gam}{Matrix of estimates of cut-off points \eqn{\gamma}, one column
 for each element of \code{lam}.}

 \item{beta}{Matrix of estimates of coefficients \eqn{\beta}, one column for
 each element of \code{lam}.}

 \item{theta}{Matrix of estimates of \eqn{\theta = [\gamma', \beta']'}.}

 \item{lam}{Vector of penalty parameters.}

 \item{iter}{Vector of number of iterations performed for each element of
 \code{lam}.}
 
 \item{conv}{Vector with convergence diagnostics for each element of
 \code{lam}: 0 means convergence, 1 means minimum was found on square root
 tolerance but \code{maxit[1]} reached, 2 means \code{maxit[1]} reached
 without finding minimum, and 3 means \code{maxit[1]} was not reached nor was
 a minimum found.}
 
 \item{err}{Vector with in-sample mis-classification rate for fitted values
 for each element of \code{lam}.}

 \item{obj}{Vector with reached objective value for each element of
 \code{lam}.}

 \item{loglik}{Vector with log-likelihood at final iterates for each element
 of \code{lam}.}

 If \code{nfold > 1}, a list with components

 \item{gam_star}{Estimate of \eqn{\gamma} at the element of \code{lam}
 selected by cross-validation.}

 \item{beta_star}{Estimate of \eqn{\beta} at the selected element of
 \code{lam}.}

 \item{theta_star}{Estimates of \eqn{\theta} at the selected element of
 \code{lam}.}

 \item{lam_star}{The selected element of \code{lam}.}

 \item{full_fit}{A list of the type returned when \code{nfold = 1}, with an
 added component \code{cv_err} which holds the cross validation
 mis-classification rate for each element of \code{lam}.}
}
\description{
{ Minimizes an elastic net-penalized negative log-likelihood for
categorical responses using accelerated proximal gradient descent or proximal
Newton. }
}
\details{
Denote the \eqn{m} levels of the response, in the order they appear when
 running \code{levels(Y)}, by \eqn{1, \dots, m}. For example, \eqn{Y_i = 1}
 means the \eqn{i}th response is equal to the first level.

 The likelihood for the \eqn{i}th observation is, for a log-concave cdf
 \eqn{R}, \deqn{R(b_i) - R(a_i),}

 where \eqn{a_i = -\infty} if \eqn{Y_i = 1} and \eqn{a_i = \sum_{j = 1}^{Y_i
 - 1}\gamma_j - x_i'\beta} otherwise; and \eqn{b_i = \infty} if \eqn{Y_i = m}
 and \eqn{b_i = \sum_{j = 1}^{Y_i}\gamma_j - x_i'\beta} otherwise.

 With the default \code{pen_factor}, the objective function minimized is
 \deqn{g(\theta; \lambda, \alpha) = -\frac{1}{n}\sum_{i = 1}^n \log\{R(b_i) -
 R(a_i)\} + \alpha \lambda \Vert \beta\Vert_1 + \frac{1}{2}(1 -
 \alpha)\lambda \Vert \beta\Vert^2,} where \eqn{\theta = [\gamma', \beta']'}.
 More generally, with \eqn{P} denoting \code{pen_factor} and \eqn{\circ} the
 elementwise product, \deqn{g(\theta; \lambda, \alpha, P) =
 -\frac{1}{n}\sum_{i = 1}^n \log\{R(b_i) - R(a_i)\} + \alpha \lambda \Vert
 P\circ \theta \Vert_1 + \frac{1}{2}(1 - \alpha)\lambda \Vert P\circ
 \theta\Vert^2.}
 
 If \code{method = "fista"}, then only the first elements of \code{maxit} and
 \code{tol} are used. If \code{method = "prox_newt"}, then the first element
 of \code{maxit} is the maximum number of Newton iterations, the second is
 the maximum number of line search iterations for each Newton update, and the
 third is the maximum number of coordinate descent iterations within each
 Newton update. The first element of \code{tol} is for terminating the Newton
 iterations and the second for terminating the coordinate descent updates
 within each Newton iteration.
}
