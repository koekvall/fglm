% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/misc.r
\name{obj_diff}
\alias{obj_diff}
\title{Evaluate objective function and its derivatives}
\usage{
obj_diff(
  y,
  X,
  b,
  yupp,
  lam = 0,
  alpha = 1,
  pen_factor = c(0, rep(1, ncol(X) - 1)),
  order
)
}
\arguments{
\item{y}{A vector of n observed responses (see details in fit_ee)}

\item{X}{An n x p matrix of predictors}

\item{b}{A vector of p regression coefficients}

\item{yupp}{A vector of n upper endpoints of intervals corresponding to y
(see details in fit_ee)}

\item{lam}{A scalar penalty parameter}

\item{alpha}{A scalar weight for elastic net (1 = lasso, 0 = ridge)}

\item{pen_factor}{A vector of coefficient-specific penalty weights; defaults
to 0 for first element of b and 1 for the remaining.}

\item{order}{An integer where 0 means only value is computed; 1 means both value
and sub-gradient; and 2 means value, sub-gradient, and Hessian (see details)}
}
\value{
A list with elements "obj", "grad", and "hessian" (see details)
}
\description{
{The objective function is that of an elastic net-penalized negative
 log-likelihood corresponding to a latent exponential generalized linear model
 with natural parameter exp(X b).
}
}
\details{
{
 When order = 0, the gradient and Hessian elements of the return list are set
 to all zeros, and similarly for the Hessian when order = 1.

 The sub-gradient returned is that obtained by taking the sub-gradient of the
 absolute value to equal zero at zero. When no element of b is zero, this is
 the usual gradient. The Hessian returns is that of the smooth part of the
 objective function that is, the average negative log-likelihood plus the L2
 penalty only. When no element of b is zero, this is the Hessian of the
 objective function
}
}
